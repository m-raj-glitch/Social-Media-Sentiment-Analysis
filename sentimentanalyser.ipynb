{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Installing Dependencies**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# utilities\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\n# plotting\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# nltk\nfrom nltk.stem import WordNetLemmatizer\n\n# sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:19:26.026951Z","iopub.execute_input":"2022-07-19T19:19:26.028125Z","iopub.status.idle":"2022-07-19T19:19:28.315617Z","shell.execute_reply.started":"2022-07-19T19:19:26.027921Z","shell.execute_reply":"2022-07-19T19:19:28.313920Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":" **Importing dataset**\n \nThe dataset being used is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets have been annotated (0 = Negative, 4 = Positive) and they can be used to detect sentiment.\n\n[The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.]\n","metadata":{}},{"cell_type":"code","source":"# Importing the dataset\nDATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv('../input/sentiment-analyser/training.1600000.processed.noemoticon.csv',\n                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n\n# Removing the unnecessary columns.\ndataset = dataset[['sentiment','text']]\n# Replacing the values to ease understanding.\ndataset['sentiment'] = dataset['sentiment'].replace(4,1)\n\n# Plotting the distribution for dataset.\nax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n                                               legend=False)\nax.set_xticklabels(['Negative','Positive'], rotation=0)\n\n# Storing data in lists.\ntext, sentiment = list(dataset['text']), list(dataset['sentiment'])","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:19:39.334058Z","iopub.execute_input":"2022-07-19T19:19:39.334545Z","iopub.status.idle":"2022-07-19T19:19:47.638290Z","shell.execute_reply.started":"2022-07-19T19:19:39.334496Z","shell.execute_reply":"2022-07-19T19:19:47.637407Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing**\n\nText Preprocessing is traditionally an important step for Natural Language Processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n\nThe Preprocessing steps taken are:\n\n1. Lower Casing: Each text is converted to lowercase.\n2. Replacing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"URL\".\n3. Replacing Emojis: Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. (eg: \":)\" to \"EMOJIsmile\")\n4. Replacing Usernames: Replace @Usernames with word \"USER\". (eg: \"@Kaggle\" to \"USER\")\n5. Removing Non-Alphabets: Replacing characters except Digits and Alphabets with a space.\n6. Removing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. (eg: \"Heyyyy\" to \"Heyy\")\n7. Removing Short Words: Words with length less than 2 are removed.\n8. Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the     meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n9. Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: “Great” to “Good”)","metadata":{}},{"cell_type":"code","source":"# Defining dictionary containing all emojis with their meanings.\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n\n## Defining set containing all stopwords in english.\nstopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n             \"youve\", 'your', 'yours', 'yourself', 'yourselves']","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:19:55.242214Z","iopub.execute_input":"2022-07-19T19:19:55.242693Z","iopub.status.idle":"2022-07-19T19:19:55.258720Z","shell.execute_reply.started":"2022-07-19T19:19:55.242656Z","shell.execute_reply":"2022-07-19T19:19:55.257238Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def preprocess(textdata):\n    processedText = []\n    \n    # Create Lemmatizer and Stemmer.\n    wordLemm = WordNetLemmatizer()\n    \n    # Defining regex patterns.\n    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n    userPattern       = '@[^\\s]+'\n    alphaPattern      = \"[^a-zA-Z0-9]\"\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    \n    for tweet in textdata:\n        tweet = tweet.lower()\n        \n        # Replace all URls with 'URL'\n        tweet = re.sub(urlPattern,' URL',tweet)\n        # Replace all emojis.\n        for emoji in emojis.keys():\n            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n        # Replace @USERNAME to 'USER'.\n        tweet = re.sub(userPattern,' USER', tweet)        \n        # Replace all non alphabets.\n        tweet = re.sub(alphaPattern, \" \", tweet)\n        # Replace 3 or more consecutive letters by 2 letter.\n        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n        tweetwords = ''\n        for word in tweet.split():\n            # Checking if the word is a stopword.\n            #if word not in stopwordlist:\n            if len(word)>1:\n                # Lemmatizing the word.\n                word = wordLemm.lemmatize(word)\n                tweetwords += (word+' ')\n            \n        processedText.append(tweetwords)\n        \n    return processedText","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:20:00.995943Z","iopub.execute_input":"2022-07-19T19:20:00.996355Z","iopub.status.idle":"2022-07-19T19:20:01.006798Z","shell.execute_reply.started":"2022-07-19T19:20:00.996309Z","shell.execute_reply":"2022-07-19T19:20:01.005415Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import time\nt = time.time()\nprocessedtext = preprocess(text)\nprint(f'Text Preprocessing complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:20:06.361824Z","iopub.execute_input":"2022-07-19T19:20:06.362198Z","iopub.status.idle":"2022-07-19T19:23:18.447135Z","shell.execute_reply.started":"2022-07-19T19:20:06.362168Z","shell.execute_reply":"2022-07-19T19:23:18.446001Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Word Cloud** \n\nNow we're going to analyse the preprocessed data to get an understanding of it. We'll plot Word Clouds for Positive and Negative tweets from our dataset and see which words occur the most.","metadata":{}},{"cell_type":"markdown","source":"**Word-Cloud for Negative tweets**","metadata":{}},{"cell_type":"code","source":"data_neg = processedtext[:800000]\nplt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n               collocations=False).generate(\" \".join(data_neg))\nplt.imshow(wc)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:24:04.381361Z","iopub.execute_input":"2022-07-19T19:24:04.381790Z","iopub.status.idle":"2022-07-19T19:24:30.753115Z","shell.execute_reply.started":"2022-07-19T19:24:04.381759Z","shell.execute_reply":"2022-07-19T19:24:30.752224Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Word-Cloud for Positive tweets**","metadata":{}},{"cell_type":"code","source":"data_pos = processedtext[800000:]\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n              collocations=False).generate(\" \".join(data_pos))\nplt.figure(figsize = (20,20))\nplt.imshow(wc)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:25:12.781718Z","iopub.execute_input":"2022-07-19T19:25:12.783921Z","iopub.status.idle":"2022-07-19T19:25:39.529755Z","shell.execute_reply.started":"2022-07-19T19:25:12.783862Z","shell.execute_reply":"2022-07-19T19:25:39.528525Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Splitting The Dataset**\n\nThe Preprocessed Data is divided into 2 sets of data:\n\nTraining Data: The dataset upon which the model would be trained on. Contains 95% data.\nTest Data: The dataset upon which the model would be tested against. Contains 5% data.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n                                                    test_size = 0.05, random_state = 0)\nprint(f'Data Split done.')","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:26:22.308659Z","iopub.execute_input":"2022-07-19T19:26:22.309139Z","iopub.status.idle":"2022-07-19T19:26:23.465104Z","shell.execute_reply.started":"2022-07-19T19:26:22.309100Z","shell.execute_reply":"2022-07-19T19:26:23.463829Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**TF-IDF**\n\nTF-IDF indicates what the importance of the word is in order to understand the document or dataset. Let us understand with an example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears many times; it’s a high frequency word compared to other words in the dataset. The dataset contains other words like home, house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the word. This is the intuition behind TF-IDF.\n\nTF-IDF Vectoriser converts a collection of raw documents to a matrix of TF-IDF features. The Vectoriser is usually trained on only the X_train dataset.\n\nngram_range is the range of number of words in a sequence. [e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]\n\nmax_features specifies the number of features to consider. [Ordered by feature frequency across the corpus].","metadata":{}},{"cell_type":"code","source":"vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint(f'Vectoriser fitted.')\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:26:34.524220Z","iopub.execute_input":"2022-07-19T19:26:34.524630Z","iopub.status.idle":"2022-07-19T19:27:57.866745Z","shell.execute_reply.started":"2022-07-19T19:26:34.524599Z","shell.execute_reply":"2022-07-19T19:27:57.865509Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Transforming the Dataset**\n\nTransforming the X_train and X_test dataset into matrix of TF-IDF Features by using the TF-IDF Vectoriser. This datasets will be used to train the models by **Sklearn** and test against it.","metadata":{}},{"cell_type":"code","source":"X_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)\nprint(f'Data Transformed.')","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:29:05.142413Z","iopub.execute_input":"2022-07-19T19:29:05.142839Z","iopub.status.idle":"2022-07-19T19:30:15.042755Z","shell.execute_reply.started":"2022-07-19T19:29:05.142807Z","shell.execute_reply":"2022-07-19T19:30:15.041413Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Creating And Evaluating Models**\n\nWe're creating 3 different types of model for our sentiment analysis problem:\n\n**Linear Support Vector Classification (LinearSVC)**\n\n**Logistic Regression (LR)**\n\n**Random Forest**","metadata":{}},{"cell_type":"code","source":"def model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:30:15.045119Z","iopub.execute_input":"2022-07-19T19:30:15.046269Z","iopub.status.idle":"2022-07-19T19:30:15.058758Z","shell.execute_reply.started":"2022-07-19T19:30:15.046217Z","shell.execute_reply":"2022-07-19T19:30:15.057414Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**LinearSVC Model**","metadata":{}},{"cell_type":"code","source":"SVCmodel = LinearSVC()\nSVCmodel.fit(X_train, y_train)\nmodel_Evaluate(SVCmodel)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:30:15.060503Z","iopub.execute_input":"2022-07-19T19:30:15.061547Z","iopub.status.idle":"2022-07-19T19:31:02.381168Z","shell.execute_reply.started":"2022-07-19T19:30:15.061497Z","shell.execute_reply":"2022-07-19T19:31:02.380076Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression Model**","metadata":{}},{"cell_type":"code","source":"LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nLRmodel.fit(X_train, y_train)\nmodel_Evaluate(LRmodel)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:31:31.911539Z","iopub.execute_input":"2022-07-19T19:31:31.912007Z","iopub.status.idle":"2022-07-19T19:37:18.616089Z","shell.execute_reply.started":"2022-07-19T19:31:31.911916Z","shell.execute_reply":"2022-07-19T19:37:18.614531Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**BernoulliNB Model**","metadata":{}},{"cell_type":"code","source":"BNBmodel = BernoulliNB(alpha = 2)\nBNBmodel.fit(X_train, y_train)\nmodel_Evaluate(BNBmodel)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:37:43.827783Z","iopub.execute_input":"2022-07-19T19:37:43.828259Z","iopub.status.idle":"2022-07-19T19:37:45.665066Z","shell.execute_reply.started":"2022-07-19T19:37:43.828222Z","shell.execute_reply":"2022-07-19T19:37:45.663748Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Accuracy**\n\n**Linear SVC(Support Vector Machines) : 82%**\n\n**Logistic Regression Model : 83%**\n\n**BernoulliNB Model : 80%**\n\nWe can clearly see that the BernoulliNB Model and Logistic Regression Model performs the best out of all the different models that we tried.But BernoulliNB Model Overfits the Data as its accuracy is  . Where as Logistic Regression Model achieves nearly 83% accuracy while classifying the sentiment of a tweet.","metadata":{}},{"cell_type":"markdown","source":"**Saving The Models**\n\nWe're using PICKLE to save Vectoriser and Logistic Regression Model for later use.","metadata":{}},{"cell_type":"code","source":"file = open('vectoriser-ngram-(1,2).pickle','wb')\npickle.dump(vectoriser, file)\nfile.close()\n\nfile = open('Sentiment-LR.pickle','wb')\npickle.dump(LRmodel, file)\nfile.close()\n\nfile = open('Sentiment-BNB.pickle','wb')\npickle.dump(BNBmodel, file)\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:38:19.452538Z","iopub.execute_input":"2022-07-19T19:38:19.453012Z","iopub.status.idle":"2022-07-19T19:38:24.618913Z","shell.execute_reply.started":"2022-07-19T19:38:19.452975Z","shell.execute_reply":"2022-07-19T19:38:24.617771Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Using The Model**\n\nTo use the model for Sentiment Prediction we need to import the Vectoriser and LR Model using Pickle.\n\nThe vectoriser can be used to transform data to matrix of TF-IDF Features. While the model can be used to predict the sentiment of the transformed Data. The text whose sentiment has to be predicted however must be preprocessed.","metadata":{}},{"cell_type":"code","source":"def load_models():\n    '''\n    Replace '..path/' by the path of the saved models.\n    '''\n    \n    # Load the vectoriser.\n    file = open('./vectoriser-ngram-(1,2).pickle', 'rb')\n    vectoriser = pickle.load(file)\n    file.close()\n    # Load the LR Model.\n    file = open('./Sentiment-LR.pickle', 'rb')\n    LRmodel = pickle.load(file)\n    file.close()\n    \n    return vectoriser, LRmodel\n\ndef predict(vectoriser, model, text):\n    # Predict the sentiment\n    textdata = vectoriser.transform(preprocess(text))\n    sentiment = model.predict(textdata)\n    \n    # Make a list of text with sentiment.\n    data = []\n    for text, pred in zip(text, sentiment):\n        data.append((text,pred))\n        \n    # Convert the list into a Pandas DataFrame.\n    df = pd.DataFrame(data, columns = ['text','sentiment'])\n    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n    return df\n\nif __name__==\"__main__\":\n    # Loading the models.\n    #vectoriser, LRmodel = load_models()\n    \n    # Text to classify should be in a list.\n    text = [\"I hate twitter\",\n            \"May the Force be with you.\",\n            \"Mr. Stark, I don't feel so good\",\n           \"I'm sorry for yesterday\"]\n    \n    df = predict(vectoriser, LRmodel, text)\n    print(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-07-19T19:43:47.245149Z","iopub.execute_input":"2022-07-19T19:43:47.245592Z","iopub.status.idle":"2022-07-19T19:43:47.267764Z","shell.execute_reply.started":"2022-07-19T19:43:47.245557Z","shell.execute_reply":"2022-07-19T19:43:47.266414Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}